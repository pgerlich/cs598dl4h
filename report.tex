%
% File acl2021.tex
%
%% Based on the style files for EMNLP 2020, which were
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2021}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

% Content lightly modified from original work by Jesse Dodge and Noah Smith


\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Reproducibility Project Instructions for CS598 DL4H in Spring 2022}

\author{Paul Gerlich \\
  \texttt{gerlich2@illinois.edu}
  \\[2em]
  Group ID: 185, Paper ID: 80\\
  Presentation link: \url{https://www.youtube.com} \\
  Code link: \url{https://www.github.com/pgerlich/cs598dl4h}} 

\begin{document}
\maketitle

% All sections are mandatory.
% Keep in mind that your page limit is 8, excluding references.
% For specific grading rubrics, please see the project instruction.

\section{Introduction}
My goal is to reproduce the findings from the paper titled "Assertion Detection in Clinical Natural Language Processing: A Knowledge-Poor Machine Learning Approach". The paper focuses on labeling assertions in clinical notes as a form of pre-processing. The purpose of the model is to correctly identify when a disease or symptom is asserted to NOT belong to the patient. A good example would be, "The patient does not show signs of heart failure". Without a semantic understanding of that assertion, a "dumb" model might just see "heart failure" and attribute it to the patient. This network intends to properly label that assertion so it can be ignored.

The interesting thing about their approach in particular is that it works without being fed knowledge about assertions. At the time of its writing, the most successful networks all had to be fed information about assertions.

\section{Scope of reproducibility}

The paper had two explorations. The primary claim was that an Attention-based BiLSTM network that was not fed prior knowledge about assertions could achieve cutting edge accuracy (F1 score of roughly 90-93 \%).

The second claim was that leveraging medical word embeddings could improve the accuracy of the model. Their findings in the second exploration did not contribute significantly to the primary claim. As such, I will take their findings (PubMed+ Word2vec is the best embedding model) as-is and use their victorious embedding model in reproducing the primary claim.

\subsection{Addressed claims from the original paper}

\begin{itemize}
    \item A knowledge poor Att-BiLSTM network can achieve comparable accuracy to a knowledge rich network
\end{itemize}


\section{Methodology}

There was no reference code for reproducing this network. I used the description of the model architecture that was outlined in the paper to reproduce their findings. I had a Mac-book Pro M1 with 16gb of RAM available to reproduce their findings. The dataset was incredibly small with roughly 500 clinical notes and less than 10k sentences. When I began, I did not believe that resource constraints would affect my ability to reproduce the results.

They did not mention any of their hyper-parameter values beyond the word embedding size (200) so I had to experiment with each network parameter to come up with a solution.

\subsection{Model descriptions}
The paper describes the Att-BiLSTM model as follows:

\textbf{Input Layer}: Represents a single sentence that uses context markers to separate the target from the rest of the sentence.

\textbf{Embedding Layer}: Each input word is translated into a one-hot word embedding vector of size = 200. The input is a sequence of size 200 vectors. The model first converts each word into a one-hot vector of total vocab size. It then converts each vector to a word embedding of real numbers (size 200) using PubMed+. The purpose of the word embedding is to represent a single word by its many similar words to allow the model to have better contextual understanding.

\textbf{BiLSTM Layer}: The embeddings are passed in for each word. Bidirectional LSTM is used to get forward and backward context within a sentence. The output is the element-wise sum of word level features.

\textbf{Attention Layer}: Word level features are multiplied by weight vector and aggregated into sentence level feature vectors.

\textbf{Output}: Classification to one of 5 labels on sentence level feature vectors.

Possible Labels
Absent (Negation) - Does not have disease,
Hypothetical - Could develop disease eventually,
Possible - Could have disease but have to rule out,
Conditional - Presents symptom IF something happens,
AWSE - Disease applies to someone else (family history, etc.),

\subsection{Data descriptions}
The paper used data from the 2010 i2b2/VA NLP challenge on relation extraction. The format of the data was a set of clinical note files split such that each line had a single sentence. Each clinical note file was supplemented with an annotation file with one annotation per line of the following format:

c="lower abdominal pain" 99:16 99:18||t="problem"||a="present"

Where c="..." denotes the subject or target of the sentence, xx:xx yy:yy denotes the line and sentence number, and a="..." denotes the label for that particular subject in that particular sentence. The t=".." data is not relevant for this challenge. I had to parse these files out, read in the clinical note for this file line by line, and generate the test and training data from these files. The paper goes a step further and actually injects context marks <c> and </c> to surround the subject in the sentence. In the end, I took roughly 500 clinical notes that translated into roughly 6000 training and 2500 test data points.

\subsection{Hyperparameters}
The original paper references another paper used the Att-BiLSTM architecture introduced by Zhou et al. [1] which I read to determine the drop out hyper parameters. They were 0.3, 0.3, and 0.5 and injected after the embedding, LSTM, and fully connected layers. 

The embedding layer translated each word into an N dimensional vector, which was chosen to be 200 as per the paper.

Finally I chose a batch size of 32, a learning rate of 1e-3, and 20 epochs to train the model. These parameters were all based on successful results with our previous class projects.

\subsection{Implementation}
The paper did not reference any code repositories so the implementation was done from the papers description of the architecture in the original paper, a published word2vec model using pubmed + wikipedia data [2], and a reference paper that outlined the original Att-BiLSTM architecture [1].

My code is located at https://github.com/pgerlich/cs598dl4h/

\subsection{Computational requirements}

I believe that I will need minimal time to train this network. I would think a quad core processor and a few compute hours at the maximum would do the trick. The challenging part of this experiment is determining the hyperparameters.

Actual results..
The word2vec model was 4gb and needed to be loaded into memory while training the network. As such, I would recommend a minimum of 8gb of memory and a 4 code CPU to reproduce this paper. The average run tim for each epoch was less than 30 seconds but I am not sure I ever got the model configured correctly.

\section{Results}
I went to great lengths just to gather and curate the dataset into the format that the author used. It was not discussed in the paper, but there was a significant amount of preprocessing that had to be done to the data. The context markers had to be injected programatically, and the labels/input data had to be manually extracted from the raw clinical notes given some rules and a custom algorithm that I had to write.

\subsection{Result 1}

F1 score achieved, if I even succeeded


\subsection{Additional results not present in the original paper}
Most of the available hyper parameters were not able to be tuned because of the rigidity of the data and embedding layer. I did mess with the batch size, number of epochs, and the learning rate to try and improve my results. 

With a batch size of 64 I noticed a significant slow down in the time to run each epoch but I actually believe that I ended up overfitting and my f1 score and ROC AUC on the test data decreased.

I also tried to tune the dropout layers from the original .3, .3, .5 to try and combat the over-fitting I thought I was experiencing. I struggled to interpret these results in a meaningful way, as the f1 and roc auc failed to increase with these changes.

\section{Discussion}

PubMed+ was no longer available so I had to use a different trained word2vec model that had the pubmed corpus in it. I found one here https://bio.nlplab.org/ which seemed to be comparable.

I believe the original paper could have been reproduced closer to when it was published. The original Pubmed+ word2vec model that they used to get the best results was also no longer available to use. I didn't understand what they had meant by using a random word embedding, either, so I was unable to effectively replicate the basic implementation of their results.

The biggest issue that I ran into with the paper itself was the way that they handled the context markers not being clearly defined. I had to guess at how to translate the context markers into a word embedding which I think impacted my results. 

Overall I think that I could have come much closer to effectively reproducing their paper if I had a better understanding of word2vec and the translation process from text data into numerical data. My knowledge in this area was relatively limited so I spent significant time struggling to understand and translate these concepts which severely diminished the amount of time I had available to try and tune the network.

\subsection{What was easy}
I was comfortable with the data processing part of the paper. It was easy to translate the data from the nlp challenge into usable input data. The model architecture was also very well described. They referenced the original paper that explained every aspect of the Att-BiLSTM model. As such, setting up the outline of the project was very easy. The way they described their overall solution was very clear and I was quickly able to outline the model.

\subsection{What was difficult}
They left gaps in their explanation of how they translate the input text into context marked text and ultimately into a word embedding vector. There are multiple interpretations of the path they chose, and I was ultimately not able to successfully perform this step which basically prevented me from running a complete model in the end. My original model that was getting 30 percent accuracy ultimately was just running on garbage data. Despite them very effectively describing their machine learning solution -
there lack of a concise description of the pre-processing steps ultimately severely impacted the reproduce-ability of the experiment. 

The other major issue was that they rely heavily on proprietary or otherwise inaccessible word2vec models that were not available to someone that wanted to reproduce the experiment. Fortunately there are similar models out there.

\subsection{Recommendations for reproducibility}
I would make the original word embedding models available for folks to use to reproduce the results. I would also describe the pre-processing step with injecting context markers more clear. How did you handle translating these markets into a vector? 

\section{Communication with original authors}
I made no attempt to contact the original authors as I was unable to find their context information.

[1] - P. Zhou, W. Shi, J. Tian, Z. Qi, B. Li, H. Hao and B. Xu, “Attention- based bidirectional long short-term memory networks for relation classification,” In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, vol. 2, 2016, pp. 207–212.
[2] - Distributional Semantics Resources for Biomedical Text Processing. Sampo Pyysalo, Filip Ginter, Hans Moen, Tapio Salakoski and Sophia Ananiadou. LBM 2013.

\bibliographystyle{acl_natbib}
\bibliography{acl2021}

%\appendix



\end{document}
