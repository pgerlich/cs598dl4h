%
% File acl2021.tex
%
%% Based on the style files for EMNLP 2020, which were
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2021}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

% Content lightly modified from original work by Jesse Dodge and Noah Smith


\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Reproducibility Project Instructions for CS598 DL4H in Spring 2022}

\author{Paul Gerlich \\
  \texttt{gerlich2@illinois.edu}
  \\[2em]
  Group ID: 185, Paper ID: 80\\
  Presentation link: \url{https://www.youtube.com} \\
  Code link: \url{https://www.github.com/pgerlich/cs598dl4h}} 

\begin{document}
\maketitle

% All sections are mandatory.
% Keep in mind that your page limit is 8, excluding references.
% For specific grading rubrics, please see the project instruction.

\section{Introduction}
My goal is to reproduce the findings from the paper titled "Assertion Detection in Clinical Natural Language Processing: A Knowledge-Poor Machine Learning Approach". The paper focuses on labeling assertions in clinical notes as a form of pre-processing. The purpose of the model is to correctly identify when a disease or symptom is asserted to NOT belong to the patient. A good example would be, "The patient does not show signs of heart failure". Without a semantic understanding of that assertion, a "dumb" model might just see "heart failure" and attribute it to the patient. This network intends to properly label that assertion so it can be ignored.

The interesting thing about their approach in particular is that it works without being fed knowledge about assertions. At the time of its writing, the most successful networks all had to be fed information about assertions.

\section{Scope of reproducibility}

The paper had two explorations. The primary claim was that an Attention-based BiLSTM network that was not fed prior knowledge about assertions could achieve cutting edge accuracy (roughly 90-93 \%).

The second claim was that leveraging meaningful medical word embeddings could improve the accuracy of the model. Their findings in the second exploration did not meaningfully contribute to the primary claim. As such, I will take their findings (PubMed+ Word2vec is the best embedding model) as-is and use their victorious embedding model in reproducing the primary claim.

\subsection{Addressed claims from the original paper}

\begin{itemize}
    \item A knowledge poor Att-BiLSTM network can achieve comparable accuracy to a knowledge rich network
    \item The PubMed+ word embedding would outperform a MIMIC III based word2vec word embedding when used in the Att-BiLSTM embedding layer
\end{itemize}


\section{Methodology}

There was no reference code for reproducing this network. I used the description of the model architecture that was outlined in the paper to reproduce their findings. I had a Mac-book Pro M1 with 16gb of RAM available to reproduce their findings. The dataset was incredibly small with roughly 500 clinical notes and less than 10k sentences. When I began, I did not believe that resource constraints would affect my ability to reproduce the results.

They did not mention any of their hyper-parameter values beyond the word embedding size (200) so I had to experiment with each network parameter to come up with a solution.

\subsection{Model descriptions}
The paper describes the Att-BiLSTM model as follows:

\textbf{Input Layer}: Represents a single sentence that uses context markers to separate the target from the rest of the sentence.

\textbf{Embedding Layer}: Each input word is translated into a one-hot word embedding vector of size = 200. The input is a sequence of size 200 vectors. The model first converts each word into a one-hot vector of total vocab size. It then converts each vector to a word embedding of real numbers (size 200) using PubMed+. The purpose of the word embedding is to represent a single word by its many similar words to allow the model to have better contextual understanding.

\textbf{BiLSTM Layer}: The embeddings are passed in for each word. Bidirectional LSTM is used to get forward and backward context within a sentence. The output is the element-wise sum of word level features.

\textbf{Attention Layer}: Word level features are multiplied by weight vector and aggregated into sentence level feature vectors.

\textbf{Output}: Classification to one of 5 labels on sentence level feature vectors.

Possible Labels
Absent (Negation) - Does not have disease,
Hypothetical - Could develop disease eventually,
Possible - Could have disease but have to rule out,
Conditional - Presents symptom IF something happens,
AWSE - Disease applies to someone else (family history, etc.),

\subsection{Data descriptions}
The paper used data from the 2010 i2b2/VA NLP challenge on relation extraction. It further supplemented the dataset with the NegEx database which is associated with a preexisting model that tried to solve the same problem. This second dataset only had a subset of available assertions (negated or not negated) vs the 5 available labels in the i2b2 dataset. Nevertheless, this dataset helped to supplement the small amount of data available for this NLP challenge.

Today, the i2b2 dataset is contained within the n2c2 dataset and is managed by the Department of Biomedical Informatics (DBMI) at Harvard Medical School. I received access to the data upon a request to their staff.

Each data entry is a full unabridged clinical note coverted to simple text. Each sentence has context markets injected to separate the sentence between the target and the rest of the sentence. An example would be: Show me the <c> target </c>. 

\subsection{Hyperparameters}
TODO: 
Describe how you set the hyperparameters and what the source was for their value (e.g. paper, code or your guess). 

\subsection{Implementation}
The paper did not reference any code repositories so the implementation was done entirely from the papers description of the architecture and trial and error with the hyper-parameters. 

\subsection{Computational requirements}

I believe that I will need minimal time to train this network. I would think a quad core processor and a few compute hours at the maximum would do the trick. The challenging part of this experiment is determining the hyperparameters.

Actual results..
I needed minimal time to train this network on a standard macbook pro with a quad core processor and 16gb of memory. I believe that this experiment could have been reproduced with half of the memory and cores.

\section{Results}
TODO: 
Start with a high-level overview of your results. Does your work support the claims you listed in section 2.1? Keep this section as factual and precise as possible, reserve your judgement and discussion points for the next ``Discussion'' section. 

Go into each individual result you have, say how it relates to one of the claims, and explain what your result is. Logically group related results into sections. Clearly state if you have gone beyond the original paper to run additional experiments and how they relate to the original claims. 

Tips 1: Be specific and use precise language, e.g. ``we reproduced the accuracy to within 1\% of reported value, that upholds the paper's conclusion that it performs much better than baselines.'' Getting exactly the same number is in most cases infeasible, so you'll need to use your judgement call to decide if your results support the original claim of the paper. 

Tips 2: You may want to use tables and figures to demonstrate your results.

% The number of subsections for results should be the same as the number of hypotheses you are trying to verify.

\subsection{Result 1}

\subsection{Result 2}

\subsection{Additional results not present in the original paper}
TODO: 
Describe any additional experiments beyond the original paper. This could include experimenting with additional datasets, exploring different methods, running more ablations, or tuning the hyperparameters. For each additional experiment, clearly describe which experiment you conducted, its result, and discussions (e.g. what is the indication of the result).

\section{Discussion}
TODO: 
Describe larger implications of the experimental results, whether the original paper was reproducible, and if it wasnâ€™t, what factors made it irreproducible. 

Give your judgement on if you feel the evidence you got from running the code supports the claims of the paper. Discuss the strengths and weaknesses of your approach -- perhaps you didn't have time to run all the experiments, or perhaps you did additional experiments that further strengthened the claims in the paper.

\subsection{What was easy}
TODO: 
Describe which parts of your reproduction study were easy. E.g. was it easy to run the author's code, or easy to re-implement their method based on the description in the paper. The goal of this section is to summarize to the reader which parts of the original paper they could easily apply to their problem. 

Tips: Be careful not to give sweeping generalizations. Something that is easy for you might be difficult to others. Put what was easy in context and explain why it was easy (e.g. code had extensive API documentation and a lot of examples that matched experiments in papers). 

\subsection{What was difficult}
TODO: 
Describe which parts of your reproduction study were difficult or took much more time than you expected. Perhaps the data was not available and you couldn't verify some experiments, or the author's code was broken and had to be debugged first. Or, perhaps some experiments just take too much time/resources to run and you couldn't verify them. The purpose of this section is to indicate to the reader which parts of the original paper are either difficult to re-use, or require a significant amount of work and resources to verify. 

Tips: Be careful to put your discussion in context. For example, don't say ``the math was difficult to follow,'' say ``the math requires advanced knowledge of calculus to follow.'' 

\subsection{Recommendations for reproducibility}
TODO: 
Describe a set of recommendations to the original authors or others who work in this area for improving reproducibility.

\section{Communication with original authors}
TODO: 

Document the extent of (or lack of) communication with the original authors. To make sure the reproducibility report is a fair assessment of the original research we recommend getting in touch with the original authors. You can ask authors specific questions, or if you don't have any questions you can send them the full report to get their feedback.


\bibliographystyle{acl_natbib}
\bibliography{acl2021}

%\appendix



\end{document}
